{
  "name": "Document Processor Tool",
  "nodes": [
    {
      "parameters": {
        "queue": "document_processing_requests",
        "options": {}
      },
      "id": "rabbitmq-consume",
      "name": "RabbitMQ Consumer",
      "type": "n8n-nodes-base.rabbitmqTrigger",
      "typeVersion": 1,
      "position": [160, 304]
    },
    {
      "parameters": {
        "jsCode": "// Document Processor Tool - Production Implementation\nconst { ChatOpenAI } = require(\"@langchain/openai\");\nconst { RecursiveCharacterTextSplitter } = require(\"langchain/text_splitter\");\nconst { Document } = require(\"@langchain/core/documents\");\n\n// Initialize LangSmith tracing\nprocess.env.LANGCHAIN_TRACING_V2 = \"true\";\nprocess.env.LANGCHAIN_PROJECT = process.env.LANGCHAIN_PROJECT || \"athenai-document-processor\";\nprocess.env.LANGCHAIN_ENDPOINT = process.env.LANGCHAIN_ENDPOINT || \"https://api.smith.langchain.com\";\n\nasync function executeDocumentProcessor() {\n  try {\n    const inputData = $json;\n    const documentUrl = inputData.document_url || inputData.url;\n    const documentContent = inputData.document_content || inputData.content;\n    const processingType = inputData.processing_type || 'extract_and_summarize';\n    const sessionId = inputData.session_id || 'default_session';\n    const toolRequestId = inputData.tool_request_id || generateRequestId();\n    \n    if (!documentUrl && !documentContent) {\n      throw new Error('Document URL or content is required');\n    }\n\n    // Get document content\n    let content = documentContent;\n    if (!content && documentUrl) {\n      content = await extractDocumentContent(documentUrl);\n    }\n    \n    if (!content) {\n      throw new Error('Unable to extract document content');\n    }\n\n    // Process document based on type\n    const processedDocument = await processDocument(content, processingType, documentUrl);\n    \n    // Generate document analysis\n    const documentAnalysis = await analyzeDocument(processedDocument, content);\n\n    const processingReport = {\n      tool_request_id: toolRequestId,\n      session_id: sessionId,\n      tool_type: \"document_processor\",\n      source_url: documentUrl,\n      processing_type: processingType,\n      processed_content: processedDocument,\n      analysis: documentAnalysis,\n      content_length: content.length,\n      processing_quality: calculateProcessingQuality(processedDocument),\n      timestamp: new Date().toISOString(),\n      status: \"completed\"\n    };\n\n    return [{\n      json: {\n        processing_report: processingReport,\n        extracted_content: processedDocument.extracted_text,\n        summary: processedDocument.summary,\n        key_points: processedDocument.key_points,\n        next_actions: determineProcessingNextActions(processedDocument),\n        neo4j_context: {\n          write: true,\n          cypher: `MERGE (s:Session {id: '${sessionId}'}) MERGE (dp:DocumentProcessing {id: '${toolRequestId}', url: '${documentUrl || 'content'}', type: '${processingType}', timestamp: datetime(), quality: ${processingReport.processing_quality}}) MERGE (s)-[:PROCESSED_DOCUMENT]->(dp)`\n        },\n        memory: {\n          upsert: true,\n          keys: [\"source_url\", \"summary\", \"key_points\", \"timestamp\"]\n        },\n        routing: {\n          queue: \"tool_results\",\n          priority: \"normal\"\n        }\n      }\n    }];\n\n  } catch (error) {\n    return [{\n      json: {\n        error: error.message,\n        tool_type: \"document_processor\",\n        status: \"failed\",\n        timestamp: new Date().toISOString(),\n        fallback_processing: {\n          message: \"Document processing failed. Manual processing may be required.\",\n          suggestions: [\n            \"Check document URL accessibility\",\n            \"Verify document format support\",\n            \"Try alternative processing methods\"\n          ]\n        }\n      }\n    }];\n  }\n}\n\n// Extract document content from URL\nasync function extractDocumentContent(url) {\n  try {\n    const response = await fetch(url, {\n      headers: {\n        'User-Agent': 'Mozilla/5.0 (compatible; AthenAI-DocumentProcessor/1.0)'\n      }\n    });\n    \n    if (!response.ok) {\n      throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n    }\n    \n    const contentType = response.headers.get('content-type') || '';\n    \n    if (contentType.includes('text/html')) {\n      return await extractHTMLContent(await response.text());\n    } else if (contentType.includes('text/plain')) {\n      return await response.text();\n    } else if (contentType.includes('application/pdf')) {\n      return await extractPDFContent(await response.arrayBuffer());\n    } else {\n      // Try to extract as text\n      return await response.text();\n    }\n    \n  } catch (error) {\n    throw new Error(`Document extraction failed: ${error.message}`);\n  }\n}\n\n// Extract content from HTML\nasync function extractHTMLContent(html) {\n  try {\n    // Remove script and style tags\n    let cleanHtml = html\n      .replace(/<script[^>]*>.*?<\\/script>/gis, '')\n      .replace(/<style[^>]*>.*?<\\/style>/gis, '')\n      .replace(/<nav[^>]*>.*?<\\/nav>/gis, '')\n      .replace(/<header[^>]*>.*?<\\/header>/gis, '')\n      .replace(/<footer[^>]*>.*?<\\/footer>/gis, '');\n    \n    // Extract text content\n    const textContent = cleanHtml\n      .replace(/<[^>]*>/g, ' ')\n      .replace(/\\s+/g, ' ')\n      .trim();\n    \n    return textContent;\n    \n  } catch (error) {\n    throw new Error(`HTML extraction failed: ${error.message}`);\n  }\n}\n\n// Extract content from PDF (simplified)\nasync function extractPDFContent(arrayBuffer) {\n  try {\n    // Note: In production, use a proper PDF parsing library like pdf-parse\n    // This is a placeholder implementation\n    return \"PDF content extraction requires additional libraries. Please provide text content directly.\";\n    \n  } catch (error) {\n    throw new Error(`PDF extraction failed: ${error.message}`);\n  }\n}\n\n// Process document based on type\nasync function processDocument(content, processingType, sourceUrl) {\n  const processed = {\n    extracted_text: content,\n    summary: '',\n    key_points: [],\n    entities: [],\n    topics: [],\n    chunks: [],\n    metadata: {\n      source: sourceUrl,\n      processing_type: processingType,\n      word_count: content.split(' ').length,\n      character_count: content.length\n    }\n  };\n\n  try {\n    // Initialize OpenAI for processing\n    const llm = new ChatOpenAI({\n      modelName: \"gpt-4\",\n      temperature: 0.1,\n      openAIApiKey: $credentials.openAi?.apiKey || process.env.OPENAI_API_KEY,\n      tags: [\"document-processor\", \"athenai\"]\n    });\n\n    // Split document into chunks\n    const textSplitter = new RecursiveCharacterTextSplitter({\n      chunkSize: 2000,\n      chunkOverlap: 200\n    });\n    \n    const chunks = await textSplitter.splitText(content);\n    processed.chunks = chunks.map((chunk, index) => ({\n      id: index,\n      content: chunk,\n      word_count: chunk.split(' ').length\n    }));\n\n    // Generate summary\n    if (processingType.includes('summarize')) {\n      processed.summary = await generateSummary(content, llm);\n    }\n\n    // Extract key points\n    if (processingType.includes('extract') || processingType.includes('key_points')) {\n      processed.key_points = await extractKeyPoints(content, llm);\n    }\n\n    // Extract entities\n    if (processingType.includes('entities')) {\n      processed.entities = await extractEntities(content, llm);\n    }\n\n    // Identify topics\n    if (processingType.includes('topics')) {\n      processed.topics = await identifyTopics(content, llm);\n    }\n\n  } catch (error) {\n    console.error('Document processing error:', error);\n    // Continue with basic processing even if AI processing fails\n    processed.summary = generateBasicSummary(content);\n    processed.key_points = extractBasicKeyPoints(content);\n  }\n\n  return processed;\n}\n\n// Generate summary using AI\nasync function generateSummary(content, llm) {\n  try {\n    const prompt = `Please provide a concise summary of the following document content:\\n\\n${content.substring(0, 4000)}...`;\n    const response = await llm.invoke(prompt);\n    return response.content || 'Summary generation failed';\n    \n  } catch (error) {\n    return generateBasicSummary(content);\n  }\n}\n\n// Extract key points using AI\nasync function extractKeyPoints(content, llm) {\n  try {\n    const prompt = `Extract the key points from the following document content. Return as a JSON array of strings:\\n\\n${content.substring(0, 4000)}...`;\n    const response = await llm.invoke(prompt);\n    \n    try {\n      return JSON.parse(response.content);\n    } catch {\n      // If JSON parsing fails, extract manually\n      return response.content.split('\\n').filter(line => line.trim().length > 10).slice(0, 5);\n    }\n    \n  } catch (error) {\n    return extractBasicKeyPoints(content);\n  }\n}\n\n// Extract entities using AI\nasync function extractEntities(content, llm) {\n  try {\n    const prompt = `Extract named entities (people, organizations, locations, dates) from the following text. Return as JSON:\\n\\n${content.substring(0, 3000)}...`;\n    const response = await llm.invoke(prompt);\n    \n    try {\n      return JSON.parse(response.content);\n    } catch {\n      return [];\n    }\n    \n  } catch (error) {\n    return [];\n  }\n}\n\n// Identify topics using AI\nasync function identifyTopics(content, llm) {\n  try {\n    const prompt = `Identify the main topics discussed in the following document. Return as a JSON array:\\n\\n${content.substring(0, 3000)}...`;\n    const response = await llm.invoke(prompt);\n    \n    try {\n      return JSON.parse(response.content);\n    } catch {\n      return response.content.split(',').map(t => t.trim()).slice(0, 5);\n    }\n    \n  } catch (error) {\n    return [];\n  }\n}\n\n// Generate basic summary (fallback)\nfunction generateBasicSummary(content) {\n  const sentences = content.split('.').filter(s => s.trim().length > 20);\n  const firstSentences = sentences.slice(0, 3).join('. ');\n  return firstSentences + (sentences.length > 3 ? '...' : '');\n}\n\n// Extract basic key points (fallback)\nfunction extractBasicKeyPoints(content) {\n  const paragraphs = content.split('\\n\\n').filter(p => p.trim().length > 50);\n  return paragraphs.slice(0, 5).map(p => p.substring(0, 100) + '...');\n}\n\n// Analyze document\nasync function analyzeDocument(processedDocument, originalContent) {\n  return {\n    content_analysis: {\n      readability: assessReadability(originalContent),\n      structure: analyzeStructure(originalContent),\n      complexity: assessComplexity(originalContent),\n      language: detectLanguage(originalContent)\n    },\n    processing_analysis: {\n      extraction_quality: assessExtractionQuality(processedDocument),\n      summary_quality: assessSummaryQuality(processedDocument.summary, originalContent),\n      key_points_relevance: assessKeyPointsRelevance(processedDocument.key_points),\n      completeness: assessProcessingCompleteness(processedDocument)\n    },\n    insights: {\n      main_themes: identifyMainThemes(processedDocument),\n      document_type: classifyDocumentType(originalContent),\n      information_density: calculateInformationDensity(originalContent)\n    }\n  };\n}\n\n// Assessment functions\nfunction assessReadability(content) {\n  const words = content.split(' ').length;\n  const sentences = content.split('.').length;\n  const avgWordsPerSentence = words / sentences;\n  \n  if (avgWordsPerSentence < 15) return 'easy';\n  if (avgWordsPerSentence < 25) return 'moderate';\n  return 'difficult';\n}\n\nfunction analyzeStructure(content) {\n  return {\n    has_headings: /^#{1,6}\\s/.test(content) || /<h[1-6]>/i.test(content),\n    has_lists: /^\\s*[-*+]\\s/.test(content) || /<[uo]l>/i.test(content),\n    has_paragraphs: content.includes('\\n\\n'),\n    paragraph_count: content.split('\\n\\n').length\n  };\n}\n\nfunction assessComplexity(content) {\n  const technicalTerms = (content.match(/\\b[A-Z]{2,}\\b/g) || []).length;\n  const longWords = content.split(' ').filter(word => word.length > 8).length;\n  const totalWords = content.split(' ').length;\n  \n  const complexityScore = (technicalTerms + longWords) / totalWords;\n  \n  if (complexityScore > 0.2) return 'high';\n  if (complexityScore > 0.1) return 'medium';\n  return 'low';\n}\n\nfunction detectLanguage(content) {\n  // Simple language detection based on common words\n  const englishWords = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'];\n  const contentLower = content.toLowerCase();\n  const englishMatches = englishWords.filter(word => contentLower.includes(` ${word} `)).length;\n  \n  return englishMatches > 5 ? 'english' : 'unknown';\n}\n\nfunction assessExtractionQuality(processedDocument) {\n  let quality = 0.5;\n  \n  if (processedDocument.extracted_text.length > 100) quality += 0.2;\n  if (processedDocument.chunks.length > 0) quality += 0.1;\n  if (processedDocument.metadata.word_count > 50) quality += 0.2;\n  \n  return Math.min(1.0, quality);\n}\n\nfunction assessSummaryQuality(summary, originalContent) {\n  if (!summary) return 0;\n  \n  const summaryLength = summary.length;\n  const originalLength = originalContent.length;\n  const compressionRatio = summaryLength / originalLength;\n  \n  // Good summary should be 5-20% of original length\n  if (compressionRatio >= 0.05 && compressionRatio <= 0.2) return 0.9;\n  if (compressionRatio >= 0.02 && compressionRatio <= 0.3) return 0.7;\n  return 0.5;\n}\n\nfunction assessKeyPointsRelevance(keyPoints) {\n  if (!keyPoints || keyPoints.length === 0) return 0;\n  \n  const avgLength = keyPoints.reduce((sum, point) => sum + point.length, 0) / keyPoints.length;\n  \n  if (avgLength >= 20 && avgLength <= 100) return 0.9;\n  if (avgLength >= 10 && avgLength <= 150) return 0.7;\n  return 0.5;\n}\n\nfunction assessProcessingCompleteness(processedDocument) {\n  let completeness = 0;\n  \n  if (processedDocument.extracted_text) completeness += 0.3;\n  if (processedDocument.summary) completeness += 0.3;\n  if (processedDocument.key_points.length > 0) completeness += 0.2;\n  if (processedDocument.chunks.length > 0) completeness += 0.1;\n  if (processedDocument.metadata) completeness += 0.1;\n  \n  return completeness;\n}\n\nfunction identifyMainThemes(processedDocument) {\n  const allText = `${processedDocument.summary} ${processedDocument.key_points.join(' ')}`;\n  const words = allText.toLowerCase().split(' ').filter(word => word.length > 4);\n  \n  // Simple frequency analysis\n  const wordFreq = {};\n  words.forEach(word => {\n    wordFreq[word] = (wordFreq[word] || 0) + 1;\n  });\n  \n  return Object.entries(wordFreq)\n    .sort(([,a], [,b]) => b - a)\n    .slice(0, 5)\n    .map(([word]) => word);\n}\n\nfunction classifyDocumentType(content) {\n  const contentLower = content.toLowerCase();\n  \n  if (contentLower.includes('abstract') && contentLower.includes('conclusion')) return 'academic_paper';\n  if (contentLower.includes('executive summary') || contentLower.includes('recommendations')) return 'business_report';\n  if (contentLower.includes('chapter') || contentLower.includes('section')) return 'book_or_manual';\n  if (contentLower.includes('article') || contentLower.includes('published')) return 'article';\n  \n  return 'general_document';\n}\n\nfunction calculateInformationDensity(content) {\n  const words = content.split(' ').length;\n  const uniqueWords = new Set(content.toLowerCase().split(' ')).size;\n  const sentences = content.split('.').length;\n  \n  return {\n    unique_word_ratio: uniqueWords / words,\n    avg_words_per_sentence: words / sentences,\n    density_score: (uniqueWords / words) * (words / sentences) / 10\n  };\n}\n\nfunction calculateProcessingQuality(processedDocument) {\n  const analysis = {\n    extraction_quality: assessExtractionQuality(processedDocument),\n    completeness: assessProcessingCompleteness(processedDocument),\n    summary_quality: processedDocument.summary ? 0.8 : 0.2,\n    key_points_quality: assessKeyPointsRelevance(processedDocument.key_points)\n  };\n  \n  const weights = {\n    extraction_quality: 0.3,\n    completeness: 0.3,\n    summary_quality: 0.2,\n    key_points_quality: 0.2\n  };\n  \n  return Object.entries(analysis).reduce((score, [key, value]) => {\n    return score + (value * weights[key]);\n  }, 0);\n}\n\nfunction determineProcessingNextActions(processedDocument) {\n  const actions = [];\n  \n  if (!processedDocument.summary) {\n    actions.push({\n      action: \"generate_summary\",\n      priority: \"medium\",\n      description: \"Document summary is missing - consider generating one\"\n    });\n  }\n  \n  if (processedDocument.key_points.length === 0) {\n    actions.push({\n      action: \"extract_key_points\",\n      priority: \"medium\",\n      description: \"No key points extracted - consider manual extraction\"\n    });\n  }\n  \n  if (processedDocument.chunks.length === 0) {\n    actions.push({\n      action: \"chunk_document\",\n      priority: \"low\",\n      description: \"Document not chunked - may affect downstream processing\"\n    });\n  }\n  \n  actions.push({\n    action: \"document_ready\",\n    priority: \"normal\",\n    description: \"Document processing completed - ready for use\"\n  });\n  \n  return actions;\n}\n\nfunction generateRequestId() {\n  return `doc_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n}\n\n// Execute and return\nreturn await executeDocumentProcessor();"
      },
      "type": "@n8n/n8n-nodes-langchain.code",
      "typeVersion": 1,
      "position": [360, 304],
      "id": "langchain-document-processor",
      "name": "Document Processor Tool"
    },
    {
      "parameters": {
        "queue": "={{ $json.routing.queue }}",
        "options": {
          "priority": "={{ $json.routing.priority }}"
        }
      },
      "id": "rabbitmq-publish",
      "name": "RabbitMQ Publish",
      "type": "n8n-nodes-base.rabbitmq",
      "typeVersion": 1,
      "position": [560, 304]
    },
    {
      "parameters": {
        "query": "={{ $json.neo4j_context.cypher }}",
        "additionalFields": {}
      },
      "id": "neo4j-write",
      "name": "Neo4j Write",
      "type": "n8n-nodes-base.neo4j",
      "typeVersion": 1,
      "position": [760, 304]
    },
    {
      "parameters": {
        "operation": "insert",
        "table": "document_processing",
        "columns": "tool_request_id, session_id, source_url, processing_type, content_length, processing_quality, timestamp",
        "values": "={{ $json.processing_report.tool_request_id }}, {{ $json.processing_report.session_id }}, {{ $json.processing_report.source_url }}, {{ $json.processing_report.processing_type }}, {{ $json.processing_report.content_length }}, {{ $json.processing_report.processing_quality }}, {{ $json.processing_report.timestamp }}"
      },
      "id": "postgres-memory",
      "name": "PostgreSQL Memory",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.4,
      "position": [960, 304]
    }
  ],
  "pinData": {},
  "connections": {
    "RabbitMQ Consumer": {
      "main": [
        [
          {
            "node": "Document Processor Tool",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Document Processor Tool": {
      "main": [
        [
          {
            "node": "RabbitMQ Publish",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RabbitMQ Publish": {
      "main": [
        [
          {
            "node": "Neo4j Write",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Neo4j Write": {
      "main": [
        [
          {
            "node": "PostgreSQL Memory",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  }
}
